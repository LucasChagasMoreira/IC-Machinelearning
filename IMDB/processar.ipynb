{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPCFYnt0gXi97m/JAL9ajzD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9FflmLbtJ5t","executionInfo":{"status":"ok","timestamp":1704143215609,"user_tz":180,"elapsed":61843,"user":{"displayName":"LUCAS CHAGAS MOREIRA","userId":"17970820584704249691"}},"outputId":"6b51e1c7-4871-455c-cbdd-888b0827e443"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n","Mounted at /content/drive\n"]}],"source":["!pip install nltk\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import nltk\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import csv\n","\n","# Baixe os recursos necessários do NLTK se ainda não tiver feito isso\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","def remove_stop_words_and_save(csv_file, column_name, output_file):\n","    stop_words = set(stopwords.words('portuguese'))  # Escolha o idioma das stop words\n","\n","    # Abre o arquivo CSV\n","    data = pd.read_csv(csv_file)\n","\n","    # Itera sobre os itens na coluna especificada\n","    for index, row in data.iterrows():\n","        text = str(row[column_name])  # Obtém o texto da coluna específica\n","        word_tokens = word_tokenize(text.lower())  # Tokeniza o texto em palavras\n","\n","        # Remove as stop words do texto tokenizado\n","        filtered_text = [word for word in word_tokens if word not in stop_words]\n","\n","        # Atualiza a coluna no DataFrame com o texto sem stop words\n","        data.at[index, column_name] = ' '.join(filtered_text)\n","\n","    # Salva o DataFrame atualizado em um novo arquivo CSV\n","    data.to_csv(output_file, index=False, quoting=csv.QUOTE_NONNUMERIC)\n","\n","# Exemplo de uso\n","remove_stop_words_and_save('/content/drive/MyDrive/sentimentogpt/IMDB Dataset.csv', 'review', 'semstopwords.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LZ3K1Uw5vdMO","executionInfo":{"status":"ok","timestamp":1704144223651,"user_tz":180,"elapsed":117661,"user":{"displayName":"LUCAS CHAGAS MOREIRA","userId":"17970820584704249691"}},"outputId":"e7196b0e-799d-4a69-8863-d5683b50ccc7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["import nltk\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import RSLPStemmer\n","import csv\n","\n","# Baixe os recursos necessários do NLTK se ainda não tiver feito isso\n","nltk.download('punkt')\n","nltk.download('rslp')\n","\n","def apply_stemming_and_save(csv_file, column_name, output_file):\n","    stemmer = RSLPStemmer()  # Escolha o algoritmo de Stemming\n","\n","    # Abre o arquivo CSV\n","    data = pd.read_csv(csv_file)\n","\n","    # Itera sobre os itens na coluna especificada\n","    for index, row in data.iterrows():\n","        text = str(row[column_name])  # Obtém o texto da coluna específica\n","        word_tokens = word_tokenize(text.lower())  # Tokeniza o texto em palavras\n","\n","        # Aplica Stemming a cada palavra do texto\n","        stemmed_text = [stemmer.stem(word) for word in word_tokens]\n","\n","        # Atualiza a coluna no DataFrame com o texto após o Stemming\n","        data.at[index, column_name] = ' '.join(stemmed_text)\n","\n","    # Salva o DataFrame atualizado em um novo arquivo CSV\n","    data.to_csv(output_file, index=False, quoting=csv.QUOTE_NONNUMERIC)\n","\n","# Exemplo de uso\n","apply_stemming_and_save('/content/drive/MyDrive/sentimentogpt/IMDB Dataset.csv', 'review', 'Imdb_stemming.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6OrAb2uWwGMi","executionInfo":{"status":"ok","timestamp":1704145068546,"user_tz":180,"elapsed":844915,"user":{"displayName":"LUCAS CHAGAS MOREIRA","userId":"17970820584704249691"}},"outputId":"886a364a-e3dc-4688-b3ed-5b06cab23cc6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package rslp to /root/nltk_data...\n","[nltk_data]   Unzipping stemmers/rslp.zip.\n"]}]},{"cell_type":"code","source":["import nltk\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import csv\n","\n","# Baixe os recursos necessários do NLTK se ainda não tiver feito isso\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","def apply_lemmatization_and_save(csv_file, column_name, output_file):\n","    lemmatizer = WordNetLemmatizer()\n","\n","    # Abre o arquivo CSV\n","    data = pd.read_csv(csv_file)\n","\n","    # Itera sobre os itens na coluna especificada\n","    for index, row in data.iterrows():\n","        text = str(row[column_name])  # Obtém o texto da coluna específica\n","        word_tokens = word_tokenize(text.lower())  # Tokeniza o texto em palavras\n","\n","        # Aplica lematização a cada palavra do texto\n","        lemmatized_text = [lemmatizer.lemmatize(word) for word in word_tokens]\n","\n","        # Atualiza a coluna no DataFrame com o texto após a lematização\n","        data.at[index, column_name] = ' '.join(lemmatized_text)\n","\n","    # Salva o DataFrame atualizado em um novo arquivo CSV\n","    data.to_csv(output_file, index=False, quoting=csv.QUOTE_NONNUMERIC)\n","\n","# Exemplo de uso\n","apply_lemmatization_and_save('/content/drive/MyDrive/sentimentogpt/IMDB Dataset.csv', 'review', 'Imdb_lemmatization.csv')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6XamigRGzC-u","executionInfo":{"status":"ok","timestamp":1704145248858,"user_tz":180,"elapsed":180319,"user":{"displayName":"LUCAS CHAGAS MOREIRA","userId":"17970820584704249691"}},"outputId":"95325b37-6109-4b5e-e631-c7ccc5740626"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}]},{"cell_type":"code","source":["import nltk\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import RSLPStemmer\n","from nltk.stem import WordNetLemmatizer\n","import csv\n","\n","# Baixe os recursos necessários do NLTK se ainda não tiver feito isso\n","nltk.download('punkt')\n","nltk.download('rslp')\n","nltk.download('wordnet')\n","\n","def apply_stemming_and_lemmatization_and_save(csv_file, column_name, output_file):\n","    stemmer = RSLPStemmer()  # Stemmer para o português\n","    lemmatizer = WordNetLemmatizer()  # Lematizador para o inglês\n","\n","    # Abre o arquivo CSV\n","    data = pd.read_csv(csv_file)\n","\n","    # Itera sobre os itens na coluna especificada\n","    for index, row in data.iterrows():\n","        text = str(row[column_name])  # Obtém o texto da coluna específica\n","        word_tokens = word_tokenize(text.lower())  # Tokeniza o texto em palavras\n","\n","        # Aplica Stemming a cada palavra do texto\n","        stemmed_text = [stemmer.stem(word) for word in word_tokens]\n","\n","        # Em seguida, aplica lematização a cada palavra do texto já stemmizado\n","        lemmatized_text = [lemmatizer.lemmatize(word) for word in stemmed_text]\n","\n","        # Atualiza a coluna no DataFrame com o texto após o Stemming e Lematização\n","        data.at[index, column_name] = ' '.join(lemmatized_text)\n","\n","    # Salva o DataFrame atualizado em um novo arquivo CSV\n","    data.to_csv(output_file, index=False, quoting=csv.QUOTE_NONNUMERIC)\n","\n","# Exemplo de uso\n","apply_stemming_and_lemmatization_and_save('/content/drive/MyDrive/sentimentogpt/IMDB Dataset.csv', 'review', 'Imbd_steeming_and_lemmatization.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kQZE4-nX1aAP","executionInfo":{"status":"ok","timestamp":1704146162304,"user_tz":180,"elapsed":913465,"user":{"displayName":"LUCAS CHAGAS MOREIRA","userId":"17970820584704249691"}},"outputId":"8aa95158-c86b-4269-84ba-c87404b4d98e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package rslp to /root/nltk_data...\n","[nltk_data]   Package rslp is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["import nltk\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import RSLPStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","import csv\n","\n","# Baixe os recursos necessários do NLTK se ainda não tiver feito isso\n","nltk.download('punkt')\n","nltk.download('rslp')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","def apply_stemming_lemmatization_remove_stopwords_and_save(csv_file, column_name, output_file):\n","    stemmer = RSLPStemmer()  # Stemmer para o português\n","    lemmatizer = WordNetLemmatizer()  # Lematizador para o inglês\n","    stop_words = set(stopwords.words('portuguese'))  # Stop words em português\n","\n","    # Abre o arquivo CSV\n","    data = pd.read_csv(csv_file)\n","\n","    # Itera sobre os itens na coluna especificada\n","    for index, row in data.iterrows():\n","        text = str(row[column_name])  # Obtém o texto da coluna específica\n","        word_tokens = word_tokenize(text.lower())  # Tokeniza o texto em palavras\n","\n","        # Remove as stop words do texto tokenizado\n","        filtered_text = [word for word in word_tokens if word not in stop_words]\n","\n","        # Aplica Stemming a cada palavra do texto já sem as stop words\n","        stemmed_text = [stemmer.stem(word) for word in filtered_text]\n","\n","        # Em seguida, aplica lematização a cada palavra do texto já stemmizado e sem stop words\n","        lemmatized_text = [lemmatizer.lemmatize(word) for word in stemmed_text]\n","\n","        # Atualiza a coluna no DataFrame com o texto após o Stemming, Lematização e remoção de stop words\n","        data.at[index, column_name] = ' '.join(lemmatized_text)\n","\n","    # Salva o DataFrame atualizado em um novo arquivo CSV\n","    data.to_csv(output_file, index=False, quoting=csv.QUOTE_NONNUMERIC)\n","\n","# Exemplo de uso\n","apply_stemming_lemmatization_remove_stopwords_and_save('/content/drive/MyDrive/sentimentogpt/IMDB Dataset.csv', 'review', 'Imbd_steeming_and_lemmatization.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iDwR-VAf1mQC","executionInfo":{"status":"ok","timestamp":1704147041119,"user_tz":180,"elapsed":878828,"user":{"displayName":"LUCAS CHAGAS MOREIRA","userId":"17970820584704249691"}},"outputId":"db04ad97-e57d-41c2-b482-4653e263aa66"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package rslp to /root/nltk_data...\n","[nltk_data]   Package rslp is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]}]}